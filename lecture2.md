---
layout: page
title: Lectures
order: 2
---

<table class="table">
          <thead>
            <tr>
              <th style="width: 12%">Date</th>
              <th style="width: 40%">Lecture</th>
              <th style="width: 15%">Homeworks</th>
              <th style="width: 30%">Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr class="active">
              <td><b>Week 1</b><br/> 
                Friday, October 4th
              </td>
              <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/01_introduction_to_machine_learning.pdf">Course introduction, problem definitions, applications</a></td>
              <td>
                <a href="material/homework_01.pdf"><b>[HW01]</b></a>
              </td>
              <td>
                <i><span class="label label-reading text-base">Reading</span></i>
              <ul>
                <li>Pedro Domingos. <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf", target="_blank">A Few Useful Things to Know About Machine Learning</a>. In: Communication of the ACM 55.10 (2012), pp. 78–87.
                </li>
                <li> D. Sculley et al. <a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf" tatget="_blank">Hidden Technical Debt in Machine Learning Systems</a>. In: 28th International Conference on Neural Information Processing Systems. 2015, pp. 2503–2511</li>
                <li><a href="http://vas3k.com/blog/machine_learning/" target="_blank">Machine Learning for Everyone</a></li>
                <li>Amershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2019/03/amershi-icse-2019_Software_Engineering_for_Machine_Learning.pdf" target="_blank">Software engineering for machine learning: a case study.</a> In 41st International Conference on Software Engineering: Software Engineering in Practice, pp. 291-300. 2019.
              </ul>
              </td>              
            </tr>
            <tr class="active">
              <td><b>Week 2</b><br/> 
                Friday, October 11th
              </td>
              <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/02_crash_course_in_python.pdf">A Crash Course in Python</a></td>
              <td>                
                <a href="material/homework_02.pdf"><b>[HW02]</b></a>
              </td>
              <td>                 
              </td>
            </tr>
            <tr class="active">
              <td><b>Week 3</b><br/> 
                Friday, October 18th
              </td>
              <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/03_scientific_computing_numpy_pandas.pdf">Scientific Computing with NumPy, Pandas, and Matplotlib</a></td>
              <td>
                <a href="material/homework_03.pdf"><b>[HW03]</b></a>
              </td>
              <td>
                <i><span class="label label-reading text-base">Reading</span></i>
                <ul>
                  <li> [Reproducible model training: deep dive](https://towardsdatascience.com/reproducible-model-training-deep-dive-2a4988d69031)</li>
                  <li> Olorisade, Babatunde K., Pearl Brereton, and Peter Andras. ["Reproducibility in machine Learning-Based studies: An example of text mining."](https://openreview.net/pdf?id=By4l2PbQ-) 2017.</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td><b>Week 4</b><br/> 
                Friday, October 25th
              </td>
              <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/04_regression_methods.pdf">Regression: predicting house prices</a> <br/>
                 <i><span class="label label-lecture text-base"><strong>Topics</strong></span></i>
                 <ul>
                   <li> Introduction </li>
                   <li> Regression, gradient descent </li> 
                   <li> Assessing performance, error types, and bias/variance trade-off </li>
                   <li>Exploratory data analysis (EDA)</li>
                 </ul>
              </td>
              <td>
                <a href="material/homework_04.pdf"><b>[HW04]</b></a>
              </td>
              <td>
              </td>
            </tr>
            <tr>
              <td><b>Week 5</b><br/> 
                Friday, November 8th
              </td>
              <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/04_regression_methods.pdf">Regression</a><br/>
                 <i><span class="label label-lecture text-base"><strong>Topics</strong></span></i>
                 <ul>
                   <li>Overfitting, regularized regression, ridge regression, lasso </li>
                   <li>Cross-validation </li>
                   <li><strong>Classification</strong></li>
                     <ul>
                       <li> Introduction </li>
                       <li> Logistic regression </li>
                     </ul>
                 </ul>
              </td>
              <td>                
              </td>
              <td>
              </td>
            </tr>
            <tr>
              <td><b>Week 5</b><br/> 
                Saturday, November 9th
              </td>
              <td><i><span class="label label-reading text-base">Lecture</span></i>                 
                <a href="slides/05_tree_based_methods.pdf">Tree-based methods</a><br/>
                 <ul>
                   <li> decision trees</li>
                   <li> overfitting in decision trees </li>
                   <li>Precision, recall, and ROC curve </li>
                   <li>Ensemble methods</li>
                     <ul>
                        <li>Boosting</li>
                        <li>Bagging</li>
                        <li>Random Forests</li>
                     </ul>
                 </ul>
              </td>
              <td>                
              </td>
              <td>                
              </td>
            </tr>            
          </tbody>
        </table>

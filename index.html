<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Introduction to Machine Learning &amp; Deep Learning </title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="Machine Learning, Deep Learning, Regression, Classification, Random Forest, Bagging Tree, ">
	<meta name="author" content="" >

	<!-- CSS -->
	<link href="/public/css/bootstrap.min.css" rel="stylesheet">
	<link href="/public/css/style.css" rel="stylesheet">
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

	<!-- Icons -->
	<!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png"> -->
	<link rel="shortcut icon" href="/public/favicon.ico">

	<!-- RSS -->
	<link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
	<script type="text/javascript" src="/public/js/jquery.min.js"></script>
	<script type="text/javascript" src="/public/js/bootstrap.min.js"></script>

	<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-HT5F7FLE2L"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-HT5F7FLE2L');
	</script>
</head>

<body>
	<div class="container">
		<div class="row clearfix">
			<div class="col-md-12 column">
				<div class="row clearfix">
					<br/>

					<h1>Introduction to Machine Learning &amp; Deep Learning </h1>
					<h3>Fall 2019, Class: Friday & Saturday from 8:30 AM to 12:30 PM</h3>
					<br/>

					<h3 id="description">Description:</h3>

                    <p>The term machine learning or statistical learning refers to the science of automated detection of patterns in data. It has been widely used in tasks that require information extraction from large data sets. Examples of tasks include SPAM detection, fraudulent credit card transaction detection, face recognition by digital cameras, and voice commands recognition by personal assistance on smart-phones. Machine learning is also widely used in scientific domains such as Bioinformatics, medicine, and astronomy. One characteristic of all these applications is that a human developer cannot provide an explicit and detailed specification of how these tasks should be executed, due to the complexity of the patterns that need to be detected.</p>

                    <br/>          			
					<h3 id="objectives">Learning Objectives:</h3>

                    <p>This is a undergraduate-level course. It provides a thorough grounding in the methods, techniques, and algorithms of machine learning. In the end of this course, the students should be able to describe the main concepts underlying machine learning, including for instance:</p>
                    <ul>
                    	<li> what is learning </li>
                    	<li> how can a machine learning </li>
                    	<li> what kind of problems can be solved by using machine learning approaches </li>
                    	<li> how to formalize them as a machine learning problem, and </li>
                    	<li> how to compare and evaluate the performance of different machine learning</li>
                    	<li> apply machine learning methods into different use cases, using Python, Pandas, scikit-learn, among others
                    </ul>

					<h3>Format</h3>

					<p>The course is a combination of theoretical and practical sessions. The theory part discusses the fundamentals of the <a href="#topics">topics</a> required to understand and design machine learning algorithms.</p>
					<br/>

					<p>This practical one implements the discussed machine learning methods using Python 3 as the programming languages and Jupyter notebook as the computing environment. </p>

					<p> All the Jupyter notebooks are available in the <a href="https://github.com/theed-ml/notebooks" target="_blank">GitHub repository</a> of the course.
					</p>					
				</div>

				<div class="github-card" data-github="theed-ml/notebooks" data-width="400" data-height="" data-theme="default"></div>
				<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>

				<h3>Grading scheme</h3>
				<ul>
					<li> A practical session that will realized in groups of two students.</li>
					<li> One individual project used for the personal evaluation at the end of the course. It must be an IoT use case.</li>
				</ul>

				<div class="hrline"> <hr /> </div> <br>
				<div class="row clearfix">
					<h3 id="textbooks">Textbooks</h2>
					<ul>
					  <li>Hastie, T., Tibshirani, R., and Friedman, J. (2016). <a href="http://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</a>. Springer, 2nd edition</li>
					  <li>Daumé III, H. (2017). <a href="http://ciml.info/dl/v0_99/ciml-v0_99-all.pdf">A Course in Machine Learning</a>. Self-published, 2nd edition</li>
					  <li>Murphy, K. P. (2012). <a href="https://www.cs.ubc.ca/~murphyk/MLbook/">Machine learning: a probabilistic perspective</a>. MIT press.</li>
					  <li>Bishop, C. M. (2006). <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern recognition and machine learning</a>. Springer.</li>
					  <li>James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). <a href="http://www-bcf.usc.edu/~gareth/ISL/">An introduction to statistical learning</a>. Springer</li>
					  <li>Goodfellow, I., Bengio, Y., and Courville, A. (2016). <a href="https://www.deeplearningbook.org/">Deep learning</a>. MIT press.</li>
					  <li>Valiant, L. (2013). Probably Approximately Correct: Nature’s Algorithms for Learning and Prospering in a Complex World. Basic Books, Inc.</li>
					</ul>
				</div>

				<h3 id="topics">Timeline</h3>
				<table class="table">
				  <thead>
				    <tr>
				      <th style="width: 12%">Date</th>
				      <th style="width: 40%">Lecture</th>
				      <th style="width: 15%">Homeworks</th>
				      <th style="width: 30%">Notes</th>
				    </tr>
				  </thead>

				  <tbody>
				    <tr>
				      <td><b>Week 1</b><br/> 
				        Friday, October 4th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/01_introduction_to_machine_learning.pdf">Course introduction, problem definitions, applications</a></td>
				      <td>
				        <a href="material/homework_01.pdf"><b>[HW01]</b></a>
				      </td>
				      <td>
				        <i><span class="label label-reading text-base">Reading</span></i> <br />
				      <ul>
				        <li>Pedro Domingos. <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank">A Few Useful Things to Know About Machine Learning</a>. In: Communication of the ACM 55.10 (2012), pp. 78–87.
				        </li>
				        <li> D. Sculley et al. <a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf" tatget="_blank">Hidden Technical Debt in Machine Learning Systems</a>. In: 28th International Conference on Neural Information Processing Systems. 2015, pp. 2503–2511</li>
				        <li><a href="http://vas3k.com/blog/machine_learning/" target="_blank">Machine Learning for Everyone</a></li>
				        <li>Amershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2019/03/amershi-icse-2019_Software_Engineering_for_Machine_Learning.pdf" target="_blank">Software engineering for machine learning: a case study.</a> In 41st International Conference on Software Engineering: Software Engineering in Practice, pp. 291-300. 2019.
				      </ul>
				      </td>              
				    </tr>
				    <tr>
				      <td><b>Week 2</b><br/> 
				        Friday, October 11th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/02_crash_course_in_python.pdf">A Crash Course in Python</a></td>
				      <td>                
				        <a href="material/homework_02.pdf"><b>[HW02]</b></a>
				      </td>
				      <td>                 
				      </td>
				    </tr>
				    <tr>
				      <td><b>Week 3</b><br/> 
				        Friday, October 18th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/03_scientific_computing_numpy_pandas.pdf">Scientific Computing with NumPy, Pandas, and Matplotlib</a></td>
				      <td>
				        <a href="#"><b>[HW03]</b></a>
				      </td>
				      <td>
				        <i><span class="label label-reading text-base">Reading</span></i> <br/>
				        <ul>
				          <li><a href="https://towardsdatascience.com/reproducible-model-training-deep-dive-2a4988d69031">Reproducible model training: deep dive</a></li>
				          <li> Olorisade, Babatunde K., Pearl Brereton, and Peter Andras. <i><a href="https://openreview.net/pdf?id=By4l2PbQ-">Reproducibility in machine Learning-Based studies: An example of text mining</a></i>. 2017.</li>
				        </ul>
				      </td>
				    </tr>
				    <tr>
				      <td><b>Week 4</b><br/> 
				        Friday, October 25th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/04_regression_methods.pdf">Regression: predicting house prices</a> <br/>
				         <!-- <i><span class="label label-lecture text-base"><strong>Topics</strong></span></i> -->
				         <ul>
				           <li> Introduction </li>
				           <li> Regression, gradient descent </li> 
				           <li> Assessing performance, error types, and bias/variance trade-off </li>
				           <li>Exploratory data analysis (EDA)</li>
				         </ul>
				      </td>
				      <td>
				        <a href="material/homework_03.pdf"><b>[HW03]</b></a>
				      </td>
				      <td>
				      </td>
				    </tr>
				    <tr>
				      <td><b>Week 5</b><br/> 
				        Friday, November 8th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i> <a href="slides/04_regression_methods.pdf">Regression</a><br/>
				         <!-- <i><span class="label label-lecture text-base"><strong>Topics</strong></span></i> -->
				         <ul>
				           <li>Overfitting, regularized regression, ridge regression, lasso </li>
				           <li>Cross-validation </li>
				           <li><strong>Classification</strong></li>
				             <ul>
				               <li> Introduction </li>
				               <li> Logistic regression </li>
				             </ul>
				         </ul>
				      </td>
				      <td>                
				      </td>
				      <td>
				      </td>
				    </tr>
				    <tr>
				      <td><b>Week 5</b><br/> 
				        Saturday, November 9th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i>                 
				        <a href="slides/05_tree_based_methods.pdf">Tree-based methods</a><br/>
				         <ul>
				           <li> decision trees</li>
				           <li> overfitting in decision trees </li>
				           <li>Precision, recall, and ROC curve </li>
				           <li>Ensemble methods</li>
				             <ul>
				                <li>Boosting</li>
				                <li>Bagging</li>
				                <li>Random Forests</li>
				             </ul>
				         </ul>
				      </td>
				      <td>                
				      </td>
				      <td> 
				      	<i><span class="label label-reading text-base">Reading</span></i> <br/>
				        <ul>
				        	<li>Quinlan, J. Ross. <a href="" target="_blank">Induction of decision trees]. Machine Learning 1, no. 1 (1986): 81-106</a>
				        	</li>
				        	<li>Breiman, Leo. <a href="https://link.springer.com/content/pdf/10.1023%2FA%3A1010933404324.pdf" target="_blank">Random forests</a>. Machine Learning 45, no. 1. pp. 5-32, 2001
				        	</li>
				        	<li>Schapire, Robert E. <a href="https://link.springer.com/content/pdf/10.1023%2FA%3A1022648800760.pdf" target="_blank"> The strength of weak learnability</a>. Machine learning 5, no. 2 pp. 197-227, 1990
				        	</li>
				        </ul>               
				      </td>
				    </tr> 
				    <tr>
				      <td><b>Week 6</b><br/> 
				        Saturday, November 16th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i>                 
				        <a href="slides/06_dimensionality_reduction.pdf">Dimensionality reduction</a><br/>
				         <ul>
				           <li> Feature selection and extraction</li>
				           <li> Principal component analysis (PCA) </li>
				           <li> t-SNE </li>
				         </ul>
				      </td>
				      <td>
				      </td>
				      <td>
				      	<i><span class="label label-reading text-base">Reading</span></i> <br/>
				         <ul>
				         	<li>Kuhn M., Johnson K. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-6849-3_19" target="_blank">An Introduction to Feature Selection</a>. In: Applied Predictive Modeling. Springer, New York, NY, 2013.</li>
				         	<li>Guyon, Isabelle, and André Elisseeff. <a href="http://jmlr.org/papers/v3/guyon03a.html" target="_blank">An introduction to variable and feature selection</a>. Journal of machine learning research 3, no. Mar (2003): 1157-1182.</li>
				         	<li>Shlens, Jonathon. <a href="https://arxiv.org/abs/1404.1100" target="_blank">A tutorial on principal component analysis</a>. arXiv preprint arXiv:1404.1100, 2014.</li>
				         	<li><a href="http://ciml.info/dl/v0_99/ciml-v0_99-ch15.pdf" target="_blank">Linear Dimensionality Reduction</a></li>
				         	<li>
				         		<a href="http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" target="_blank"> Visualizing data using t-SNE</a>
				         	</li>
				         	<li>
				         		<a href="https://distill.pub/2016/misread-tsne" target="_blank">How to Use t-SNE Effectively</a>
				         	</li>
				         	<li> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" target="_blank">sklearn.decomposition.PCA</a></li>
				         </ul>               
				      </td>
				    </tr>
				    <tr class="active">
				      <td><b>Week 7</b><br/> 
				        Friday, November 22th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i>                 
				        <a href="slides/07_similarity_and_clustering_methods.pdf">Clustering and similarity</a><br/>
				         <ul>
				           <li>Introduction to clustering</li>
				           <li>Nearest neighbors methods </li>
				           <li>k-means clustering</li>
				         </ul>
				      </td>
				      <td>                
				      </td>
				      <td>                
				      </td>
				    </tr>
				    <tr>
				      <td><b>Week 7</b><br/> 
				        Saturday, November 23th
				      </td>
				      <td><i><span class="label label-reading text-base">Reading</span></i> Application of machine learning to estimate energy performance of residential buildings. <br /> <br />

				      	<ul>
				      		<li>Tsanas A, Xifara A. <a href="http://people.maths.ox.ac.uk/tsanas/Preprints/ENB2012.pdf" target="_blank"><em>Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools</em></a>. Energy and Buildings. 2012 Jun 1;49:560-7.</li>
				      		<li>Johannesen, Nils Jakob, Kolhe, Mohan, et Goodwin, Morten. <a href="https://www.sciencedirect.com/science/article/pii/S0959652619301192" target="_blank">Relative evaluation of regression tools for urban area electrical energy demand forecasting</a>. Journal of cleaner production, 2019, vol. 218, p. 555-564.</li>
				      	</ul>
				      </td>
				      <td>

				      	 <td>
				      	 	<i><span class="label label-due text-base">Due</span></i><a href="material/homework_04.pdf"><b>[CW04]</b></a> (Deadline at November 23th, 12:30 PM GMT+1)<br>
				      	 	<p> Submit your analysis (i.e., Jupyter notebook) through the following form.</p>
				      	 	<a href="https://forms.gle/H5hM6Ho9CBc1WppA7" target="_blank">Submission form</a>
				      </td>
				      </td>
				      <td>                
				      </td>
				    </tr>
				    <tr>
				      <td><b>Week 8</b><br/> 
				        Saturday, November 30th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i>

				      	<a href="#">Neural networks</a><br/>
				      	 <ul>
				      	   <li>Perceptron</li>
				      	   <li>Multilayer Perceptron</li>
				      	   <li>Support Vector Machines (SVM)</li>
				      	 </ul>
				      </td>
				      <td>
				      </td>
				      <td>                
				      </td>
				    </tr>
				    <tr>
				      <td><b>Week 9</b><br/> 
				        Saturday, December 7th
				      </td>
				      <td><i><span class="label label-lecture text-base">Lecture</span></i> Exam<br />
				      </td>
				      <td>
				      </td>
				      <td>                
				      </td>
				    </tr>
				    <tr>
				      <td><b>Week 10</b><br/> 
				        Saturday, December 14th
				      </td>
				      <td>				      	
				      	<i><span class="label label-due text-base">Due</span></i> Delivery and presentation of the projects <br />
				      	<ul>
				      		<li><a href="https://github.com/theed-ml/report_manuscript_template" target="_blank">Structure and template LaTeX of the report</a> &nbsp;&nbsp; [<a href="material/structure_report_manuscript.pdf" target="_blank">PDF</a>]</li>
				      		<!-- <li><a href="" target="_blank">Structure and template LaTeX of the presentation</a></li> -->
				      	</ul>
				      	Everyone will have <strong>ten minutes</strong> to present his/her work. 
				      </td>
				      <td>
				      </td>
				      <td>                
				      </td>
				    </tr>
				  </tbody>          
				</table>

				<br>
				<div class="hrline"> <hr /> </div>
				<br>
				</div>
			</div>

			<div class="row clearfix">
			  <h2>Resources</h2>
				<h3 id="datasets">Dataset repositories</h3>
				  <ul>
				  	<li><a href="https://archive.ics.uci.edu/ml/index.php" target="_blank">UC Irvine Machine Learning Repository</a></li>
				  	<li><a href="https://kaggle.com/datasets">Kaggle datasets</a></li>
				  	<li><a href="https://aws.amazon.com/datasets" target="_blank">Amazon's AWS datasets</a></li>
				  	<li><a href="http://vlado.fmf.uni-lj.si/pub/networks/data/">Pajek datasets</a></li>
				  	<li><a href="https://www.quandl.com">Quandl</a></li>
				  	<li><a href="https://toolbox.google.com/datasetsearch">Google Dataset Search</a></li>
				  </ul>

				  <h3 id="cheatsheets">Cheat Sheets</h3>

				    <ul>
				    	<li> <a href="http://datacamp-community-prod.s3.amazonaws.com/dbed353d-2757-4617-8206-8767ab379ab3" target="_blank">Pandas Basics</a></li>
				    	<li> <a href="https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf" target="_blank">Data Wrangling with pandas Cheat Sheet</a></li>
				    	<li> <a href="https://s3.amazonaws.com/dq-blog-files/numpy-cheat-sheet.pdf" target="_blank">NumPy Cheat Sheet</a></li>			    	
				    </ul>

				   <h3 id="documentations">Documentations</h3>
				    <ul>
				    	<li> <a href="https://docs.scipy.org/doc/numpy/" target="_blank">NumPy</a></li>
				    	<li> <a href="http://pandas.pydata.org/pandas-docs/stable/index.html" target="_blank">pandas</a></li>
				    	<li> <a href="https://scikit-learn.org/stable/documentation.html" target="_blank">scikit-learn</a></li>
				    	<li> <a href="http://web.mit.edu/dvp/Public/numpybook.pdf" target="_blank">Guide to NumPy</a></li>
				    </ul>

				  <h3 id="environments">Online Jupyter environment</h3>
				    <ul>
				    	<li> <a href="https://colab.research.google.com/" target="_blank">Google Colab</a></li>
				    </ul>

				  <h3 id="newsletters">Newsletters</h3>

				  <ul>
				  	<li> <a href="https://cds.nyu.edu/newsletter" target="_blank">YU Data Science Newsletter</a></li>
				  	<li> <a href="https://www.deeplearning.ai/thebatch" target="_blank">The Batch</a></li>
				  	<li><a href="https://groups.google.com/forum/#!forum/ml-news" target="_blank">ML News</a></li>
				  </ul>

				  <h3 id="podcasts">Podcasts</h3>
				    <ul>
				    	<li> <a href="https://www.microsoft.com/en-us/research/blog/category/podcast" target="_blank">Microsoft Research Podcast</a></li>
				    	<li> <a href="https://thetalkingmachines.com" target="_blank">Talking Machines</a></li>
				    	<li><a href="https://twimlai.com" target="_blank">TWiMLAI</a></li>
				    	<li> <a href="http://nssdeviations.com" target="_blank">Not So Standard Deviation</a></li>
				    	<li> <a href="http://lineardigressions.com" target="_blank">Linear Digressions</a></li>
				    	<li> <a href="https://podcasts.ox.ac.uk/series/futuremakers" target="_blank">Futuremakers</a></li>
				    	<li><a href="https://overcast.fm/itunes1259235188/women-in-business-technology" target="_blank">Women in Business &amp; Technology</a></li>
				    </ul>			
			</div>

			<div class="hrline"><hr></div> <br />
			<h2 id="contact">Contact</h1>
			<div class="row clearfix">
				<div class="col-md-3 column">					
					<form action="https://formspree.io/xdqnlldm" method="POST">
					  Name: <input type="text" name="name" placeholder="Your name" size="30"> <br />
					  Email: <input type="email" name="_replyto" placeholder="Your email" size="40"> <br />
					  <p> Message: </p> 
					  <textarea name="message" placeholder="Type your message" rows="20" cols="60"></textarea>
					  <p>
					  <input type="submit" value="Send" class="">
					</p>
					  <input type="hidden" name="_subject" value="ESME"/>
					  <input type="hidden" name="_next" value="thanks.html"/>
					</form>
				</div>			   
			</div>

		</div>

		<!-- The footer -->
		<!-- <div class="hrline"><hr></div><br> -->
		<!-- <div class="row clearfix">
			<div class="col-md-3 column">
			</div>
			<div class="col-md-6 column">
				<div class = "text-center">
					<p>
					&nbsp;&nbsp;&nbsp; &#169;
					</p>
				</div>
			</div>
			<div class="col-md-3 column">
			</div>
		</div> -->
		<!-- <br>
		<br>
		<br> -->
		<!-- End of document -->
	</div>
</body>
</html>

